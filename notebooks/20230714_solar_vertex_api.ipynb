{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "import os\n",
    "import json\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_custom_trained_model_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instances: Union[Dict, List[Dict]],\n",
    "    location: str = \"europe-west9\",\n",
    "    api_endpoint: str = \"europe-west9-aiplatform.googleapis.com\",\n",
    "):\n",
    "    \"\"\"\n",
    "    `instances` can be either single instance of type dict or a list\n",
    "    of instances.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    \n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    \n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    instances = instances if type(instances) == list else [instances]\n",
    "    \n",
    "    instances = [\n",
    "        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "    ]\n",
    "    \n",
    "    parameters_dict = {}\n",
    "    \n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    \n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    \n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    \n",
    "    print(\"response\")\n",
    "    \n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    \n",
    "    # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "    \n",
    "    predictions = response.predictions\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        print\n",
    "        print(\" prediction:\", list(prediction))\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPrecondition",
     "evalue": "400 The request size (347850812 bytes) exceeds 1.500MB limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/grpc/_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1028\u001b[0m state, call, \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m   1029\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m-> 1030\u001b[0m \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/grpc/_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"The request size (347850812 bytes) exceeds 1.500MB limit.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2a00:1450:4001:806::200a%5D:443 {grpc_message:\"The request size (347850812 bytes) exceeds 1.500MB limit.\", grpc_status:9, created_time:\"2023-07-13T16:11:28.559314+02:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../vertex/solar/test_input_solar.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m     instances \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[0;32m----> 8\u001b[0m predictions \u001b[39m=\u001b[39m predict_custom_trained_model_sample(\n\u001b[1;32m      9\u001b[0m     project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m1088859348322\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     endpoint_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m2201222278807552000\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39meurope-west9\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     instances\u001b[39m=\u001b[39;49minstances\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../vertex/solar/predictions.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     16\u001b[0m     json\u001b[39m.\u001b[39mdump(predictions, file,\n\u001b[1;32m     17\u001b[0m                 indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, sort_keys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mpredict_custom_trained_model_sample\u001b[0;34m(project, endpoint_id, instances, location, api_endpoint)\u001b[0m\n\u001b[1;32m     38\u001b[0m parameters \u001b[39m=\u001b[39m json_format\u001b[39m.\u001b[39mParseDict(parameters_dict, Value())\n\u001b[1;32m     40\u001b[0m endpoint \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mendpoint_path(\n\u001b[1;32m     41\u001b[0m     project\u001b[39m=\u001b[39mproject, location\u001b[39m=\u001b[39mlocation, endpoint\u001b[39m=\u001b[39mendpoint_id\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     45\u001b[0m     endpoint\u001b[39m=\u001b[39;49mendpoint, instances\u001b[39m=\u001b[39;49minstances, parameters\u001b[39m=\u001b[39;49mparameters\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m deployed_model_id:\u001b[39m\u001b[39m\"\u001b[39m, response\u001b[39m.\u001b[39mdeployed_model_id)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:602\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    597\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(metadata) \u001b[39m+\u001b[39m (\n\u001b[1;32m    598\u001b[0m     gapic_v1\u001b[39m.\u001b[39mrouting_header\u001b[39m.\u001b[39mto_grpc_metadata(((\u001b[39m\"\u001b[39m\u001b[39mendpoint\u001b[39m\u001b[39m\"\u001b[39m, request\u001b[39m.\u001b[39mendpoint),)),\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    601\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[1;32m    603\u001b[0m     request,\n\u001b[1;32m    604\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m    605\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    606\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:74\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 The request size (347850812 bytes) exceeds 1.500MB limit."
     ]
    }
   ],
   "source": [
    "# Set the environment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../vertex/helios-ecosolutions-94ab5d2658af.json\"\n",
    "\n",
    "# Load the input data from a local JSON file\n",
    "with open(\"../vertex/solar/test_input_solar.json\", \"r\") as file:\n",
    "    instances = json.load(file)\n",
    "    \n",
    "predictions = predict_custom_trained_model_sample(\n",
    "    project=\"1088859348322\",\n",
    "    endpoint_id=\"2201222278807552000\",\n",
    "    location=\"europe-west9\",\n",
    "    instances=instances\n",
    ")\n",
    "\n",
    "with open(\"../vertex/solar/predictions.json\", \"w\") as file:\n",
    "    json.dump(predictions, file,\n",
    "                indent=4, sort_keys=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
