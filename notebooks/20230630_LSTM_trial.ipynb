{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approach to Long-Short-Term Memory model #\n",
    " https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "import statsmodels.graphics.tsaplots as tsaplot\n",
    "from statsmodels.tsa.holtwinters import Holt, ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential # intitialize the ANN\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM     # create layers\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the test train split \n",
    "in our case we can create several shorter sequences that we will use to train our model with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/final_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_names(df):\n",
    "    column_names = {'Photovoltaics [MWh] Original resolutions': 'Solar_generation_MWh',\n",
    "                'Photovoltaics [MW] Calculated resolutions': 'Solar_installed_MW',\n",
    "                'Total (grid load) [MWh] Original resolutions': 'Total_consumption_MWh',\n",
    "                'Germany/Luxembourg [â‚¬/MWh] Calculated resolutions': 'DE_LU_price_per_MWh',}\n",
    "    df.rename(columns=column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163680 entries, 0 to 163679\n",
      "Data columns (total 7 columns):\n",
      " #   Column                           Non-Null Count   Dtype         \n",
      "---  ------                           --------------   -----         \n",
      " 0   Date                             163680 non-null  datetime64[ns]\n",
      " 1   Solar_generation_MWh             163680 non-null  float64       \n",
      " 2   Solar_installed_MW               163680 non-null  float64       \n",
      " 3   Total_consumption_MWh            163680 non-null  float64       \n",
      " 4   DE_LU_price_per_MWh              163680 non-null  float64       \n",
      " 5   normalisation_factor             163680 non-null  float64       \n",
      " 6   Solar_generation_MWh_normalized  163680 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6)\n",
      "memory usage: 8.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a lot of samples therefore I will limit the sample size for training a bit mor\n",
    "#df = df.iloc[60000: , :]\n",
    "#len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df['Solar_generation_MWh_normalized'], test_size=.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(input, n_steps, pred_size):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(input)):\n",
    "        end_ix = i + n_steps # find the end of this pattern\n",
    "        if end_ix+pred_size > len(input)-1: # check if we are beyond the sequence\n",
    "            break\n",
    "        seq_x, seq_y = input[i:end_ix], input[end_ix: end_ix+pred_size]# gather input and output parts of the pattern\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113808 113808\n"
     ]
    }
   ],
   "source": [
    "# define input sequence\n",
    "input = train\n",
    "# choose a number of time steps\n",
    "n_steps = 672\n",
    "\n",
    "# prediction size \n",
    "pred_size= 96\n",
    "# split into samples\n",
    "X, y = split_sequence(input, n_steps, pred_size)\n",
    "# summarize the data\n",
    "print(len(X), len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113808, 96) (113808, 672)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have to define the validation set for our model \n",
    "def val_set(X,y):\n",
    "    train_size = round(len(X) * 0.8)\n",
    "    X = X[:train_size, :]\n",
    "    X_val = X[train_size:, :]\n",
    "    y = y[:train_size, :]\n",
    "    y_val = y[train_size:, :]\n",
    "    return X, X_val, y, y_val\n",
    "X, X_val, y, y_val = val_set(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91046, 672)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "\n",
    "def reshape_for_LSTM(X, y, features):\n",
    "    n_features = features\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "    y = y.reshape((y.shape[0], y.shape[1]))\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets start the modeling approach using the Long short term memory model ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary to store results\n",
    "training_history = {}\n",
    "\n",
    "# Define number of epochs and learning rate decay\n",
    "N_TRAIN = len(X)\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 2371 # total sample size = 113808 each batch 2371 samples (48 batches ) #! has to be adjusted further to improve\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.01,\n",
    "    decay_steps=STEPS_PER_EPOCH*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "\n",
    "\n",
    "# Define optimizer used for modelling\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule, name='Adam')  # due to a warning message I used the legacy.Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path where checkpoints should be stored\n",
    "checkpoint_path = \"modeling/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0) # Set verbose != 0 if you want output during training \n",
    "\n",
    "cp_early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=0, verbose=0,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=False,\n",
    "                                start_from_epoch=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how many output layer are needed for predicting several timestamps? Please check one output layer is enough but some of the parameters have to be adjusted,\n",
    "\n",
    "n_steps, n_features\n",
    "X.shape[1], X.shape[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reason for not having activation functions https://datascience.stackexchange.com/questions/66594/activation-function-between-lstm-layers\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell\n",
    "\n",
    "output layer structure : https://stackoverflow.com/questions/46797891/output-shape-of-lstm-model#46799544\n",
    "\n",
    "https://shiva-verma.medium.com/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_LSTM_model():\n",
    "    simple_LSTM = tf.keras.Sequential([\n",
    "      tf.keras.layers.LSTM(45 ,kernel_initializer = 'uniform', input_shape = (X.shape[1], X.shape[2]), return_sequences=True), # ! units are not set in stone yet \n",
    "      tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(y.shape[1] ,kernel_initializer = 'uniform', activation='relu') #96 to predict a day \n",
    "    ])\n",
    "\n",
    "    simple_LSTM.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error')\n",
    "    return simple_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 672, 45)           8460      \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 32)                9984      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 96)                3168      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21612 (84.42 KB)\n",
      "Trainable params: 21612 (84.42 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    simple_LSTM = get_simple_LSTM_model()\n",
    "    print(simple_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Epoch 2/200\n",
      "Epoch 3/200\n",
      "Epoch 4/200\n",
      "Epoch 5/200\n",
      "Epoch 6/200\n",
      "Epoch 7/200\n",
      "Epoch 8/200\n",
      "Epoch 9/200\n",
      "Epoch 10/200\n",
      "Epoch 11/200\n",
      "Epoch 12/200\n",
      "Epoch 13/200\n",
      "Epoch 14/200\n",
      "Epoch 15/200\n",
      "Epoch 16/200\n",
      "Epoch 17/200\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    training_history['small'] = simple_LSTM.fit(X,\n",
    "                        y,\n",
    "                        batch_size= BATCH_SIZE,\n",
    "                        validation_data= (X_val, y_val),   ##### probably best to make validation data D #! TO DO \n",
    "                        verbose=100,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[cp_callback, cp_early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we split the test set too \n",
    "X_test, y_test = split_sequence(test, n_steps, pred_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2054.61789898, 2210.56216942, 2347.96923089, ...,  953.7394014 ,\n",
       "        1104.35422426, 1229.01695457],\n",
       "       [2210.56216942, 2347.96923089, 2453.3996069 , ..., 1104.35422426,\n",
       "        1229.01695457, 1355.06997555],\n",
       "       [2347.96923089, 2453.3996069 , 2530.79245434, ..., 1229.01695457,\n",
       "        1355.06997555, 1466.29322936],\n",
       "       [2453.3996069 , 2530.79245434, 2569.48887806, ..., 1355.06997555,\n",
       "        1466.29322936, 1566.39415778],\n",
       "       [2530.79245434, 2569.48887806, 2576.90376165, ..., 1466.29322936,\n",
       "        1566.39415778, 1660.4704933 ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "than we take only the first element of the splited test set and let the model predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.35506998e+03, 1.46629323e+03, 1.56639416e+03, 1.66047049e+03,\n",
       "        1.75454683e+03, 1.83564712e+03, 1.87527040e+03, 1.87619726e+03,\n",
       "        1.86113578e+03, 1.82846395e+03, 1.77192546e+03, 1.68943488e+03,\n",
       "        1.58076049e+03, 1.44335343e+03, 1.28370172e+03, 1.10667138e+03,\n",
       "        9.23153007e+02, 7.40561498e+02, 5.63067722e+02, 3.90439964e+02,\n",
       "        2.32178542e+02, 1.11686684e+02, 3.75378482e+01, 5.09773247e+00,\n",
       "        4.63430224e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.08543601e+00, 3.10498250e+01, 1.28138457e+02, 2.69021245e+02,\n",
       "        4.36319556e+02, 6.23313652e+02, 8.27686380e+02, 1.02811995e+03,\n",
       "        1.22021178e+03, 1.39724213e+03, 1.56708930e+03, 1.73369247e+03],\n",
       "       [1.46629323e+03, 1.56639416e+03, 1.66047049e+03, 1.75454683e+03,\n",
       "        1.83564712e+03, 1.87527040e+03, 1.87619726e+03, 1.86113578e+03,\n",
       "        1.82846395e+03, 1.77192546e+03, 1.68943488e+03, 1.58076049e+03,\n",
       "        1.44335343e+03, 1.28370172e+03, 1.10667138e+03, 9.23153007e+02,\n",
       "        7.40561498e+02, 5.63067722e+02, 3.90439964e+02, 2.32178542e+02,\n",
       "        1.11686684e+02, 3.75378482e+01, 5.09773247e+00, 4.63430224e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.08543601e+00,\n",
       "        3.10498250e+01, 1.28138457e+02, 2.69021245e+02, 4.36319556e+02,\n",
       "        6.23313652e+02, 8.27686380e+02, 1.02811995e+03, 1.22021178e+03,\n",
       "        1.39724213e+03, 1.56708930e+03, 1.73369247e+03, 1.87897784e+03]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[286.59402 286.51938 286.21994 286.3898  286.82928 286.6774  286.6389\n",
      "  286.62173 286.60873 286.71832 286.03122 286.46057 286.98767 286.65497\n",
      "  286.83405 286.72284 286.67163 286.51608 286.373   286.36923 284.6028\n",
      "  286.73822 286.2747  286.38876 286.65762 286.2997  286.22675 286.55508\n",
      "  286.5328  286.74207 286.80722 286.4979  286.49902 286.41113 286.55402\n",
      "  286.18497 286.5143  286.07092 285.95804 286.0895  286.61246 286.2862\n",
      "  286.26508 286.37247 286.17657 286.38394 286.69855 286.91843 286.17145\n",
      "  286.5744  286.4122  287.00052 286.71686 286.7545  286.21356 286.88184\n",
      "  286.3336  286.3371  286.48718 286.40094 286.33313 286.92947 286.74484\n",
      "  286.7728  286.30743 286.43207 286.03055 286.84692 287.18433 286.5848\n",
      "  286.82327 286.9135  286.32715 286.46082 286.0757  286.6289    0.\n",
      "  286.81787 286.19464 287.01535 287.04318 286.89453 286.9978  286.67902\n",
      "  286.39923 286.80142 287.03888 286.86307   0.      286.69598 286.5787\n",
      "  286.66776 286.5675  286.59763 286.85474 286.52054]\n",
      " [286.59402 286.51938 286.21994 286.3898  286.82928 286.6774  286.6389\n",
      "  286.62173 286.60873 286.71832 286.03122 286.46057 286.98767 286.65497\n",
      "  286.83405 286.72284 286.67163 286.51608 286.373   286.36923 284.6028\n",
      "  286.73822 286.2747  286.38876 286.65762 286.2997  286.22675 286.55508\n",
      "  286.5328  286.74207 286.80722 286.4979  286.49902 286.41113 286.55402\n",
      "  286.18497 286.5143  286.07092 285.95804 286.0895  286.61246 286.2862\n",
      "  286.26508 286.37247 286.17657 286.38394 286.69855 286.91843 286.17145\n",
      "  286.5744  286.4122  287.00052 286.71686 286.7545  286.21356 286.88184\n",
      "  286.3336  286.3371  286.48718 286.40094 286.33313 286.92947 286.74484\n",
      "  286.7728  286.30743 286.43207 286.03055 286.84692 287.18433 286.5848\n",
      "  286.82327 286.9135  286.32715 286.46082 286.0757  286.6289    0.\n",
      "  286.81787 286.19464 287.01535 287.04318 286.89453 286.9978  286.67902\n",
      "  286.39923 286.80142 287.03888 286.86307   0.      286.69598 286.5787\n",
      "  286.66776 286.5675  286.59763 286.85474 286.52054]]\n"
     ]
    }
   ],
   "source": [
    "x_input = X_test[:2, :]\n",
    "x_input = x_input.reshape((x_input.shape[0], x_input.shape[1], n_features))\n",
    "y_pred = simple_LSTM.predict(x_input, verbose=0)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_mse(y_test, y_pred):\n",
    "    return keras.metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([440084.16, 455714.1 ], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_mse(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short summary ###\n",
    "model is performing shitty it looks like it cannot handle the seasonality at all "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach get rif of the seasonality beforehand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Solar_detrend'] = (df['Solar_generation_MWh_normalized'] - df['Solar_generation_MWh_normalized'].shift(96))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163680 entries, 0 to 163679\n",
      "Data columns (total 8 columns):\n",
      " #   Column                           Non-Null Count   Dtype         \n",
      "---  ------                           --------------   -----         \n",
      " 0   Date                             163680 non-null  datetime64[ns]\n",
      " 1   Solar_generation_MWh             163680 non-null  float64       \n",
      " 2   Solar_installed_MW               163680 non-null  float64       \n",
      " 3   Total_consumption_MWh            163680 non-null  float64       \n",
      " 4   DE_LU_price_per_MWh              163680 non-null  float64       \n",
      " 5   normalisation_factor             163680 non-null  float64       \n",
      " 6   Solar_generation_MWh_normalized  163680 non-null  float64       \n",
      " 7   Solar_detrend                    163584 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(7)\n",
      "memory usage: 10.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113808 113808\n"
     ]
    }
   ],
   "source": [
    "#Split into test and train set\n",
    "train, test = train_test_split(df['Solar_detrend'], test_size=.3, shuffle=False)\n",
    "\n",
    "# define input sequence\n",
    "input = train\n",
    "# choose a number of time steps (a week)\n",
    "n_steps = 672\n",
    "\n",
    "# prediction size (we want to predict a day)\n",
    "pred_size= 96\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(input, n_steps, pred_size)\n",
    "# summarize the data\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train set into train and validation set\n",
    "X, X_val, y, y_val = val_set(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = reshape_for_LSTM(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_LSTM_model():\n",
    "    wo_season_LSTM = tf.keras.Sequential([\n",
    "      tf.keras.layers.LSTM(45 ,kernel_initializer = 'uniform', input_shape = (X.shape[1], X.shape[2]), return_sequences=True), # ! units are not set in stone yet \n",
    "      tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(y.shape[1] ,kernel_initializer = 'uniform', activation='relu') #96 to predict a day \n",
    "    ])\n",
    "\n",
    "    wo_season_LSTM.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error')\n",
    "    return wo_season_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 672, 45)           8460      \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 32)                9984      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 96)                3168      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21612 (84.42 KB)\n",
      "Trainable params: 21612 (84.42 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    wo_season_LSTM = get_season_LSTM_model()\n",
    "    print(wo_season_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Epoch 2/200\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    training_history['season_free'] = wo_season_LSTM.fit(X,\n",
    "                        y,\n",
    "                        batch_size= BATCH_SIZE,\n",
    "                        validation_data= (X_val, y_val),   ##### probably best to make validation data D #! TO DO \n",
    "                        verbose=100,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[cp_callback, cp_early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [nan, nan], 'mse': [nan, nan]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history['season_free'].history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# short Summary \n",
    "sadly I still only get nan for loss and mse. check with this in detail \n",
    "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 48336\n  y sizes: 2\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEvaluate on test data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m results \u001b[39m=\u001b[39m wo_season_LSTM\u001b[39m.\u001b[39;49mevaluate(X_test, y_test, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2371\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest loss, test acc:\u001b[39m\u001b[39m\"\u001b[39m, results)\n",
      "File \u001b[0;32m~/git/NeueFische/EES_Optimisation/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/git/NeueFische/EES_Optimisation/.venv/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1950\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1943\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1944\u001b[0m         label,\n\u001b[1;32m   1945\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m   1946\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1947\u001b[0m         ),\n\u001b[1;32m   1948\u001b[0m     )\n\u001b[1;32m   1949\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1950\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 48336\n  y sizes: 2\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = wo_season_LSTM.evaluate(X_test, y_test, batch_size=2371)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
