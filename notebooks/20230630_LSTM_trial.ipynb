{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approach to a univariate Long-Short-Term Memory model for predicting the solar output  #\n",
    "\n",
    "For our optimisation we need solar output predictions. In this notebook we will use a univariate Long-Short-Term Memory model to predict the solar output. \n",
    "\n",
    "\n",
    " https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "import statsmodels.graphics.tsaplots as tsaplot\n",
    "from statsmodels.tsa.holtwinters import Holt, ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential # intitialize the ANN\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM , Conv1D, MaxPooling1D  # create layers\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with loading the pickle file with our full dataset into this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/final_dataframe.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are not very easy to work with and can be a bit hard to read. Therefore we will rename them to make them easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_names(df):\n",
    "    ''' this function renames the columns to make them easier to read \n",
    "      additionally set the date as index in our dataframe'''\n",
    "    column_names = {'Photovoltaics [MWh] Original resolutions': 'Solar_generation_MWh',\n",
    "                'Photovoltaics [MW] Calculated resolutions': 'Solar_installed_MW',\n",
    "                'Total (grid load) [MWh] Original resolutions': 'Total_consumption_MWh',\n",
    "                'Germany/Luxembourg [â‚¬/MWh] Calculated resolutions': 'DE_LU_price_per_MWh',}\n",
    "    df.rename(columns=column_names, inplace=True)\n",
    "    #df.set_index('Date', inplace=True)\n",
    "    return df\n",
    "\n",
    "col_names(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can already split the data into train and test set. Important to note here is that the shuffle has to be false otherwise the split is not appropriate for time series analysis. I will use the previously defined approach from the 20230704_train_val_test_split notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have a huge problem with the 0 therfore I will add 1 to all my datapoints \n",
    "#df['Solar_generation_MWh_normalized'] = df['Solar_generation_MWh_normalized'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['Date', 'Solar_generation_MWh_normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_timeseries(df):\n",
    "    ''' In the first part we select the train and test data.\n",
    "    In the second per the columns we want to use for our predictions '''\n",
    "    \n",
    "    test = df[df.Date >= '2022-06-01']\n",
    "    train = df[df.Date < '2022-06-01']\n",
    "\n",
    "    # now we select the columns we want to use for our predictions\n",
    "\n",
    "    test = test[target]\n",
    "    train = train[target]\n",
    "    return test, train\n",
    "\n",
    "test, train = test_train_timeseries(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively we could also use \n",
    "#train, test = train_test_split(df[['Solar_generation_MWh_normalized']], test_size=.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's scale the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data for the model #! Test MinMaxScaler \n",
    "\n",
    "scalabe = ['Solar_generation_MWh_normalized']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train[scalabe] = scaler.fit_transform(train[scalabe])\n",
    "test[scalabe] = scaler.transform(test[scalabe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old version ....\n",
    " split a univariate sequence into samller samples to feed into the LSTM\n",
    "def split_sequence(input, n_steps, pred_size):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(input)):\n",
    "        end_ix = i + n_steps # find the end of this pattern\n",
    "        if end_ix+pred_size > len(input)-1: # check if we are beyond the sequence\n",
    "            break\n",
    "        seq_x, seq_y = input[i:end_ix], input[end_ix: end_ix+pred_size]# gather input and output parts of the pattern\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.squeeze(np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samller samples to feed into the LSTM\n",
    "def split_sequence(input, n_steps, pred_size, target = []):\n",
    "    ''' This function will split our timeseries into supervised timeseries snipets. \n",
    "    input = dataframe to be split\n",
    "    n_steps = length of the X_variable \n",
    "    pred_size = length of the y_variable\n",
    "    features = list of targets to be split\n",
    "    At the same time we will collect the corresponding timestamps in two additional arrays '''\n",
    "    input_array = input[target]\n",
    "    date_array = input['Date']\n",
    "\n",
    "    x_index, y_index = list(), list()\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(input_array)):\n",
    "        end_ix = i + n_steps # find the end of this pattern\n",
    "        if end_ix+pred_size > len(input)-1: # check if we are beyond the sequence\n",
    "            break\n",
    "        seq_x, seq_y = input_array[i:end_ix], input_array[end_ix: end_ix+pred_size]# gather input and output parts of the pattern\n",
    "        ind_x, ind_y = date_array[i:end_ix], date_array[end_ix: end_ix+pred_size]# gather input and output Dates of the pattern\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        x_index.append(ind_x)\n",
    "        y_index.append(ind_y)\n",
    "\n",
    "    \n",
    "    return np.array(x), np.squeeze(np.array(y)), np.array(x_index), np.squeeze(np.array(y_index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input sequence\n",
    "input = train\n",
    "# choose a number of time steps\n",
    "n_steps = 672\n",
    "# prediction size \n",
    "pred_size= 96\n",
    "\n",
    "target = ['Solar_generation_MWh_normalized']\n",
    "\n",
    "# split into samples\n",
    "X, y, X_train_index, Y_train_index = split_sequence(input, n_steps, pred_size, target)\n",
    "# summarize the data\n",
    "print(len(X), len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)\n",
    "\n",
    "print(X_train_index.shape, Y_train_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, X_test_index, Y_test_index = split_sequence(test , n_steps, pred_size, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(X_test_index.shape, Y_test_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have to define the validation set for our model #! I see this approach is not so useful, therefore I will use the train test split with shuffling to obtain the validation data. Here i am not loosing the lateest data for training my model \n",
    "def val_set(X,y):\n",
    "    X, X_val, y, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    return X, X_val, y, y_val\n",
    "    #! old approach\n",
    "    #train_size = round(len(X) * 0.8)\n",
    "    #X = X[:train_size, :]\n",
    "    #X_val = X[train_size:, :]\n",
    "    #y = y[:train_size, :]\n",
    "    #y_val = y[train_size:, :]\n",
    "    \n",
    "X, X_val, y, y_val = val_set(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    " #! correction still necessary\n",
    "\n",
    "def reshape_for_LSTM(X, y, features):\n",
    "    features\n",
    "    X = X.reshape((X.shape[0], X.shape[1], features))\n",
    "    y = y.reshape((y.shape[0], y.shape[1]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = reshape_for_LSTM(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = reshape_for_LSTM(X_val, y_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = reshape_for_LSTM(X_test, y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets start the modeling approach using the Long short term memory model ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary to store results\n",
    "history = {}\n",
    "\n",
    "# Define number of epochs and learning rate decay\n",
    "N_TRAIN = len(X)\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 2600 # total sample size = 97593 each batch 2600 samples (49 batches ) #! has to be adjusted further to improve\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay( \n",
    "    0.001,  #! please adjust and finetune ? should be fine like this \n",
    "    decay_steps=STEPS_PER_EPOCH*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "\n",
    "\n",
    "# Define optimizer used for modelling\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule, name='Adam')  # due to a warning message I used the legacy.Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path where checkpoints should be stored\n",
    "checkpoint_path = \"modeling/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0) # Set verbose != 0 if you want output during training \n",
    "\n",
    "#create a callback to stop early once there is no improvement in the loss\n",
    "cp_early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min',\n",
    "                                restore_best_weights=True,\n",
    "                                verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how many output layer are needed for predicting several timestamps? Please check one output layer is enough but some of the parameters have to be adjusted,\n",
    "\n",
    "n_steps, n_features\n",
    "X.shape[1], X.shape[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reason for not having activation functions https://datascience.stackexchange.com/questions/66594/activation-function-between-lstm-layers\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell\n",
    "\n",
    "output layer structure : https://stackoverflow.com/questions/46797891/output-shape-of-lstm-model#46799544\n",
    "\n",
    "https://shiva-verma.medium.com/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import Dense, Activation, Dropout, LSTM , Conv1D, MaxPooling1D, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_LSTM_model():\n",
    "    simple_LSTM = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv1D(32, kernel_size = 5, activation ='relu', input_shape =(X.shape[1], X.shape[2])),\n",
    "      tf.keras.layers.MaxPooling1D(pool_size = 2),\n",
    "      tf.keras.layers.LSTM(45, kernel_initializer = 'uniform', return_sequences=True), # ! units are not set in stone yet \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(y.shape[1] ,kernel_initializer = 'uniform', activation='linear') #96 to predict a day \n",
    "    ])\n",
    "\n",
    "    simple_LSTM.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.MeanAbsolutePercentageError(), \n",
    "                  metrics=[tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "    return simple_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    simple_LSTM = get_simple_LSTM_model()\n",
    "    print(simple_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    history = simple_LSTM.fit(X,\n",
    "                        y,\n",
    "                        batch_size= BATCH_SIZE,\n",
    "                        validation_data= (X_val, y_val),   ##### probably best to make validation data D #! TO DO \n",
    "                        verbose=10,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                        epochs=EPOCHS,\n",
    "                        shuffle = False, \n",
    "                        callbacks=[cp_callback, cp_early_stop]) # try without early stopping to see if there is something wrong cp_early_stop #!patience=5 helps to aviod getting stuck in local minima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device('/cpu:0'):\n",
    "#    simple_LSTM_reloaded = tf.keras.models.load_model('saved_model/simple_LSTM')\n",
    "STOP\n",
    "# Check its architecture\n",
    "#simple_LSTM_reloaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = simple_LSTM.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (simple_LSTM.metrics_names[1], scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = X_test\n",
    "#x_input = x_input.reshape((x_input.shape[0], x_input.shape[1], 1))\n",
    "y_pred = simple_LSTM.predict(x_input, verbose=0)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = simple_LSTM.evaluate(X_test, y_test, batch_size=2600)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! simple_LSTM.save('../models/saved_model/simple_LSTM_70error')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now make a new timeseries from out predicted values so we can plot them nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get all the original timestamps from our original test dataset. We splitted this dataset into features and target. \n",
    "# we used the first 672 entries to predict the next 96 so for our predicted values we will start only from index 672.\n",
    "# The following timestamps will be predicted by our model \n",
    "df_trial = test.iloc[672:].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we make a new column containing the first prediction of our\n",
    "df_trial['predicted_values'] = y_pred[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['Solar_generation_MWh_normalized_inverse_transformed'] = scaler.inverse_transform(df_trial['Solar_generation_MWh_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are still not very good. It seems that there are some problems with handeling the 0 and also the seasonality. More work is needed. \n",
    "    Option 1: I will remove the seasonality from the data before it goes into the model \n",
    "    Option 2: I will include the price and weather data to make the model more robust. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now create a proper output table which can then be used for the Optimisation ###\n",
    "Now we will transform out output back into the same units as before and add them to a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_and_frame(X, y, X_test_index):\n",
    "    ''' The input are both our arrays X_test and the predicted y and the date index of the original X_test array.\n",
    "\n",
    "     1st. Inverse transfrom the arrays to the original dimensions needed by the optimizer \n",
    "     2nd. Create a dataframe for both x_test and y_pred \n",
    "     3rd. Merge all the columns representing the timesteps into a single array in one column \n",
    "     4th. Create an empty dataframe and add the last date of X_test_index as the timepoint from which the prediction started\n",
    "     5th  Concatenate the two dataframes into one \n",
    "    '''\n",
    "    inversed_y_pred = scaler.inverse_transform(y)\n",
    "    inversed_X_test = scaler.inverse_transform(X.reshape(X.shape[0], X.shape[1]))\n",
    "\n",
    "    X_test = pd.DataFrame(inversed_X_test)\n",
    "    y_pred = pd.DataFrame(inversed_y_pred)\n",
    "\n",
    "    X_test['input_array'] = X_test.apply(lambda row: np.array(row), axis=1)\n",
    "    y_pred['output_array'] = y_pred.apply(lambda row: np.array(row), axis=1)\n",
    "\n",
    "    df_final = pd.DataFrame()\n",
    "    df_final[\"Date\"]= X_test_index[:, -1]\n",
    "\n",
    "    df_final['output'] = y_pred['output_array']\n",
    "    df_final['input'] = X_test['input_array']\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_predictions= reverse_and_frame(X_test, y_pred, X_test_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include the date and time column back to the output dataframe ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solar_predictions = solar_predictions[['Date', 'output']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solar_predictions.to_pickle(\"../predictions/solar_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire small model as a SavedModel.\n",
    "#!mkdir -p ../models/saved_model\n",
    "#simple_LSTM.save('../models/saved_model/simple_LSTM_2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short summary ###\n",
    "model is performing poorly, the overall MAPE is still around 65 % error which is helpful for our approach. Based on the output I is not able to handle the 0 very well. I will now look a bit deeper into the 0 problem. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach get rid of the seasonality beforehand \n",
    "\n",
    "TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
