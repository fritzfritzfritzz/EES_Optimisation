{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approach to Long-Short-Term Memory model #\n",
    " https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install protobuf\n",
    "#%pip install tensorflow-macos\n",
    "#%pip install tensorflow-metal\n",
    "#%pip install Keras\n",
    "#%pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "import statsmodels.graphics.tsaplots as tsaplot\n",
    "from statsmodels.tsa.holtwinters import Holt, ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential # intitialize the ANN\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM     # create layers\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the test train split \n",
    "in our case we can create several shorter sequences that we will use to train our model with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/final_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_names(df):\n",
    "    column_names = {'Photovoltaics [MWh] Original resolutions': 'Solar_generation_MWh',\n",
    "                'Photovoltaics [MW] Calculated resolutions': 'Solar_installed_MW',\n",
    "                'Total (grid load) [MWh] Original resolutions': 'Total_consumption_MWh',\n",
    "                'Germany/Luxembourg [â‚¬/MWh] Calculated resolutions': 'DE_LU_price_per_MWh',}\n",
    "    df.rename(columns=column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163680 entries, 0 to 163679\n",
      "Data columns (total 7 columns):\n",
      " #   Column                           Non-Null Count   Dtype         \n",
      "---  ------                           --------------   -----         \n",
      " 0   Date                             163680 non-null  datetime64[ns]\n",
      " 1   Solar_generation_MWh             163680 non-null  float64       \n",
      " 2   Solar_installed_MW               163680 non-null  float64       \n",
      " 3   Total_consumption_MWh            163680 non-null  float64       \n",
      " 4   DE_LU_price_per_MWh              163680 non-null  float64       \n",
      " 5   normalisation_factor             163680 non-null  float64       \n",
      " 6   Solar_generation_MWh_normalized  163680 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6)\n",
      "memory usage: 8.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df['Solar_generation_MWh_normalized'], test_size=.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(input, n_steps, pred_size):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(input)):\n",
    "        end_ix = i + n_steps # find the end of this pattern\n",
    "        if end_ix+pred_size > len(input)-1: # check if we are beyond the sequence\n",
    "            break\n",
    "        seq_x, seq_y = input[i:end_ix], input[end_ix: end_ix+pred_size]# gather input and output parts of the pattern\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113808 113808\n"
     ]
    }
   ],
   "source": [
    "# define input sequence\n",
    "input = train\n",
    "# choose a number of time steps\n",
    "n_steps = 672\n",
    "\n",
    "# prediction size \n",
    "pred_size= 96\n",
    "# split into samples\n",
    "X, y = split_sequence(input, n_steps, pred_size)\n",
    "# summarize the data\n",
    "print(len(X), len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113808, 96) (113808, 672)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91046"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(X) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have to define the validation set for our model \n",
    "def val_set(X,y):\n",
    "    train_size = round(len(X) * 0.8)\n",
    "    X = X[:train_size, :]\n",
    "    X_val = X[train_size:, :]\n",
    "    y = y[:train_size, :]\n",
    "    y_val = y[train_size:, :]\n",
    "    return X, X_val, y, y_val\n",
    "X, X_val, y, y_val = val_set(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "y = y.reshape((y.shape[0], y.shape[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets start the modeling approach using the Long short term memory model ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary to store results\n",
    "training_history = {}\n",
    "\n",
    "# Define number of epochs and learning rate decay\n",
    "N_TRAIN = len(X)\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = 2371 # total sample size = 113808 each batch 2371 samples (48 batches ) #! has to be adjusted further to improve\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.01,\n",
    "    decay_steps=STEPS_PER_EPOCH*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "\n",
    "\n",
    "# Define optimizer used for modelling\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule, name='Adam')  # due to a warning message I used the legacy.Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(history):\n",
    "    plt.plot(history.history['mse'])\n",
    "    plt.plot(history.history['val_mse'])\n",
    "    plt.title('Model MSE')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path where checkpoints should be stored\n",
    "checkpoint_path = \"modeling/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0) # Set verbose != 0 if you want output during training \n",
    "\n",
    "cp_early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=False,\n",
    "                                start_from_epoch=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how many output layer are needed for predicting several timestamps? Please check one output layer is enough but some of the parameters have to be adjusted,\n",
    "\n",
    "n_steps, n_features\n",
    "X.shape[1], X.shape[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reason for not having activation functions https://datascience.stackexchange.com/questions/66594/activation-function-between-lstm-layers\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_LSTM_model():\n",
    "    simple_LSTM = tf.keras.Sequential([\n",
    "      tf.keras.layers.LSTM(units = 45 ,kernel_initializer = 'uniform', input_shape = (X.shape[1], X.shape[2]), return_sequences=True), # ! units are not set in stone yet \n",
    "      tf.keras.layers.LSTM(32, activation='relu', return_sequences=False),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(y.shape[1] ,kernel_initializer = 'uniform', activation='relu' ) #96 to predict a day \n",
    "    ])\n",
    "\n",
    "    simple_LSTM.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mse'])\n",
    "    return simple_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 672, 45)           8460      \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 32)                9984      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 96)                3168      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,612\n",
      "Trainable params: 21,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    simple_LSTM = get_simple_LSTM_model()\n",
    "    print(simple_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 18:47:27.156388: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    training_history['small'] = simple_LSTM.fit(X,\n",
    "                        y,\n",
    "                        batch_size= BATCH_SIZE,\n",
    "                        validation_data= (X_val, y_val),   ##### probably best to make validation data D #! TO DO \n",
    "                        verbose=0,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[cp_callback, cp_early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we split the test set too \n",
    "X_test, y_test = split_sequence(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "than we take only the first element of the splited test set and let the model predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = test\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "y_pred = model.predict(x_input, verbose=0)\n",
    "print(ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
