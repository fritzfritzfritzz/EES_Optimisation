{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approach to Long-Short-Term Memory model #\n",
    " https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "import statsmodels.graphics.tsaplots as tsaplot\n",
    "from statsmodels.tsa.holtwinters import Holt, ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential # intitialize the ANN\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM     # create layers\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the test train split \n",
    "in our case we can create several shorter sequences that we will use to train our model with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/final_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_names(df):\n",
    "    column_names = {'Photovoltaics [MWh] Original resolutions': 'Solar_generation_MWh',\n",
    "                'Photovoltaics [MW] Calculated resolutions': 'Solar_installed_MW',\n",
    "                'Total (grid load) [MWh] Original resolutions': 'Total_consumption_MWh',\n",
    "                'Germany/Luxembourg [€/MWh] Calculated resolutions': 'DE_LU_price_per_MWh',}\n",
    "    df.rename(columns=column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Solar_generation_MWh</th>\n",
       "      <th>Solar_installed_MW</th>\n",
       "      <th>Total_consumption_MWh</th>\n",
       "      <th>DE_LU_price_per_MWh</th>\n",
       "      <th>normalisation_factor</th>\n",
       "      <th>Solar_generation_MWh_normalized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-01 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>59.53</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01 00:15:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>59.53</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01 00:30:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>59.53</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01 00:45:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>59.53</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>56.10</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-01 22:45:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>62579.0</td>\n",
       "      <td>12945.50</td>\n",
       "      <td>95.41</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-01 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>62579.0</td>\n",
       "      <td>12817.75</td>\n",
       "      <td>86.53</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-01 23:15:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>62579.0</td>\n",
       "      <td>12539.00</td>\n",
       "      <td>86.53</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-01 23:30:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>62579.0</td>\n",
       "      <td>12371.00</td>\n",
       "      <td>86.53</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-01 23:45:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>62579.0</td>\n",
       "      <td>12176.75</td>\n",
       "      <td>86.53</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163680 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Solar_generation_MWh  Solar_installed_MW   \n",
       "Date                                                            \n",
       "2018-10-01 00:00:00                   0.0             42805.0  \\\n",
       "2018-10-01 00:15:00                   0.0             42805.0   \n",
       "2018-10-01 00:30:00                   0.0             42805.0   \n",
       "2018-10-01 00:45:00                   0.0             42805.0   \n",
       "2018-10-01 01:00:00                   0.0             42805.0   \n",
       "...                                   ...                 ...   \n",
       "2023-06-01 22:45:00                   0.0             62579.0   \n",
       "2023-06-01 23:00:00                   0.0             62579.0   \n",
       "2023-06-01 23:15:00                   0.0             62579.0   \n",
       "2023-06-01 23:30:00                   0.0             62579.0   \n",
       "2023-06-01 23:45:00                   0.0             62579.0   \n",
       "\n",
       "                     Total_consumption_MWh  DE_LU_price_per_MWh   \n",
       "Date                                                              \n",
       "2018-10-01 00:00:00               10589.75                59.53  \\\n",
       "2018-10-01 00:15:00               10589.75                59.53   \n",
       "2018-10-01 00:30:00               10589.75                59.53   \n",
       "2018-10-01 00:45:00               10589.75                59.53   \n",
       "2018-10-01 01:00:00               10589.75                56.10   \n",
       "...                                    ...                  ...   \n",
       "2023-06-01 22:45:00               12945.50                95.41   \n",
       "2023-06-01 23:00:00               12817.75                86.53   \n",
       "2023-06-01 23:15:00               12539.00                86.53   \n",
       "2023-06-01 23:30:00               12371.00                86.53   \n",
       "2023-06-01 23:45:00               12176.75                86.53   \n",
       "\n",
       "                     normalisation_factor  Solar_generation_MWh_normalized  \n",
       "Date                                                                        \n",
       "2018-10-01 00:00:00              0.684015                              0.0  \n",
       "2018-10-01 00:15:00              0.684015                              0.0  \n",
       "2018-10-01 00:30:00              0.684015                              0.0  \n",
       "2018-10-01 00:45:00              0.684015                              0.0  \n",
       "2018-10-01 01:00:00              0.684015                              0.0  \n",
       "...                                   ...                              ...  \n",
       "2023-06-01 22:45:00              1.000000                              0.0  \n",
       "2023-06-01 23:00:00              1.000000                              0.0  \n",
       "2023-06-01 23:15:00              1.000000                              0.0  \n",
       "2023-06-01 23:30:00              1.000000                              0.0  \n",
       "2023-06-01 23:45:00              1.000000                              0.0  \n",
       "\n",
       "[163680 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Solar_generation_MWh</th>\n",
       "      <th>Solar_installed_MW</th>\n",
       "      <th>Total_consumption_MWh</th>\n",
       "      <th>DE_LU_price_per_MWh</th>\n",
       "      <th>normalisation_factor</th>\n",
       "      <th>Solar_generation_MWh_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-01 00:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>59.53</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-01 00:15:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>59.53</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-01 00:30:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>59.53</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-01 00:45:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>59.53</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-01 01:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10589.75</td>\n",
       "      <td>56.10</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-10-01 01:15:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10560.25</td>\n",
       "      <td>56.10</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-10-01 01:30:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10507.00</td>\n",
       "      <td>56.10</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-10-01 01:45:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10407.50</td>\n",
       "      <td>56.10</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-10-01 02:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10263.50</td>\n",
       "      <td>51.41</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-10-01 02:15:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10274.00</td>\n",
       "      <td>51.41</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-10-01 02:30:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10370.50</td>\n",
       "      <td>51.41</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-10-01 02:45:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10366.50</td>\n",
       "      <td>51.41</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-10-01 03:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10456.25</td>\n",
       "      <td>47.38</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-10-01 03:15:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10521.75</td>\n",
       "      <td>47.38</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-10-01 03:30:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10548.50</td>\n",
       "      <td>47.38</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-10-01 03:45:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10569.00</td>\n",
       "      <td>47.38</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-10-01 04:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10706.00</td>\n",
       "      <td>47.59</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-10-01 04:15:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10835.25</td>\n",
       "      <td>47.59</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-10-01 04:30:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>10998.25</td>\n",
       "      <td>47.59</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-10-01 04:45:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>11119.50</td>\n",
       "      <td>47.59</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-10-01 05:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>11503.50</td>\n",
       "      <td>51.61</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-10-01 05:15:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>11767.75</td>\n",
       "      <td>51.61</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-10-01 05:30:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>12134.25</td>\n",
       "      <td>51.61</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-10-01 05:45:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>12623.25</td>\n",
       "      <td>51.61</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-10-01 06:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>13506.25</td>\n",
       "      <td>69.13</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-10-01 06:15:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>14138.00</td>\n",
       "      <td>69.13</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-10-01 06:30:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>14659.75</td>\n",
       "      <td>69.13</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-10-01 06:45:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>15086.50</td>\n",
       "      <td>69.13</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-10-01 07:00:00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>15599.50</td>\n",
       "      <td>77.32</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1.197027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-10-01 07:15:00</td>\n",
       "      <td>14.75</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>15808.50</td>\n",
       "      <td>77.32</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>10.089227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-10-01 07:30:00</td>\n",
       "      <td>56.75</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>15941.00</td>\n",
       "      <td>77.32</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>38.817874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-10-01 07:45:00</td>\n",
       "      <td>135.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16122.50</td>\n",
       "      <td>77.32</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>92.342080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-10-01 08:00:00</td>\n",
       "      <td>255.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16292.25</td>\n",
       "      <td>84.97</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>174.423928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-10-01 08:15:00</td>\n",
       "      <td>407.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16500.00</td>\n",
       "      <td>84.97</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>278.394270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-10-01 08:30:00</td>\n",
       "      <td>588.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16668.75</td>\n",
       "      <td>84.97</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>402.201058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-10-01 08:45:00</td>\n",
       "      <td>759.50</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16724.75</td>\n",
       "      <td>84.97</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>519.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-10-01 09:00:00</td>\n",
       "      <td>958.50</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16596.75</td>\n",
       "      <td>79.56</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>655.628765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-10-01 09:15:00</td>\n",
       "      <td>1190.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16705.25</td>\n",
       "      <td>79.56</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>813.978331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-10-01 09:30:00</td>\n",
       "      <td>1388.75</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16819.25</td>\n",
       "      <td>79.56</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>949.926393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-10-01 09:45:00</td>\n",
       "      <td>1602.25</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16883.00</td>\n",
       "      <td>79.56</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1095.963682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-10-01 10:00:00</td>\n",
       "      <td>1795.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16901.25</td>\n",
       "      <td>73.70</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1227.807651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-10-01 10:15:00</td>\n",
       "      <td>2002.50</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>16997.00</td>\n",
       "      <td>73.70</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1369.740848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-10-01 10:30:00</td>\n",
       "      <td>2214.75</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>17077.50</td>\n",
       "      <td>73.70</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1514.923117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-10-01 10:45:00</td>\n",
       "      <td>2351.50</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>17206.25</td>\n",
       "      <td>73.70</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1608.462224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-10-01 11:00:00</td>\n",
       "      <td>2419.75</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>17295.75</td>\n",
       "      <td>71.63</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1655.146275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-10-01 11:15:00</td>\n",
       "      <td>2523.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>17457.50</td>\n",
       "      <td>71.63</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1725.770866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-10-01 11:30:00</td>\n",
       "      <td>2622.75</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>17472.00</td>\n",
       "      <td>71.63</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1794.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-10-01 11:45:00</td>\n",
       "      <td>2673.50</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>17440.00</td>\n",
       "      <td>71.63</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1828.715184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018-10-01 12:00:00</td>\n",
       "      <td>2751.75</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>17409.50</td>\n",
       "      <td>63.15</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1882.239389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2018-10-01 12:15:00</td>\n",
       "      <td>2779.00</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>17342.75</td>\n",
       "      <td>63.15</td>\n",
       "      <td>0.684015</td>\n",
       "      <td>1900.878809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date  Solar_generation_MWh  Solar_installed_MW   \n",
       "0  2018-10-01 00:00:00                  0.00             42805.0  \\\n",
       "1  2018-10-01 00:15:00                  0.00             42805.0   \n",
       "2  2018-10-01 00:30:00                  0.00             42805.0   \n",
       "3  2018-10-01 00:45:00                  0.00             42805.0   \n",
       "4  2018-10-01 01:00:00                  0.00             42805.0   \n",
       "5  2018-10-01 01:15:00                  0.00             42805.0   \n",
       "6  2018-10-01 01:30:00                  0.00             42805.0   \n",
       "7  2018-10-01 01:45:00                  0.00             42805.0   \n",
       "8  2018-10-01 02:00:00                  0.00             42805.0   \n",
       "9  2018-10-01 02:15:00                  0.00             42805.0   \n",
       "10 2018-10-01 02:30:00                  0.00             42805.0   \n",
       "11 2018-10-01 02:45:00                  0.00             42805.0   \n",
       "12 2018-10-01 03:00:00                  0.00             42805.0   \n",
       "13 2018-10-01 03:15:00                  0.00             42805.0   \n",
       "14 2018-10-01 03:30:00                  0.00             42805.0   \n",
       "15 2018-10-01 03:45:00                  0.00             42805.0   \n",
       "16 2018-10-01 04:00:00                  0.00             42805.0   \n",
       "17 2018-10-01 04:15:00                  0.00             42805.0   \n",
       "18 2018-10-01 04:30:00                  0.00             42805.0   \n",
       "19 2018-10-01 04:45:00                  0.00             42805.0   \n",
       "20 2018-10-01 05:00:00                  0.00             42805.0   \n",
       "21 2018-10-01 05:15:00                  0.00             42805.0   \n",
       "22 2018-10-01 05:30:00                  0.00             42805.0   \n",
       "23 2018-10-01 05:45:00                  0.00             42805.0   \n",
       "24 2018-10-01 06:00:00                  0.00             42805.0   \n",
       "25 2018-10-01 06:15:00                  0.00             42805.0   \n",
       "26 2018-10-01 06:30:00                  0.00             42805.0   \n",
       "27 2018-10-01 06:45:00                  0.00             42805.0   \n",
       "28 2018-10-01 07:00:00                  1.75             42805.0   \n",
       "29 2018-10-01 07:15:00                 14.75             42805.0   \n",
       "30 2018-10-01 07:30:00                 56.75             42805.0   \n",
       "31 2018-10-01 07:45:00                135.00             42805.0   \n",
       "32 2018-10-01 08:00:00                255.00             42805.0   \n",
       "33 2018-10-01 08:15:00                407.00             42805.0   \n",
       "34 2018-10-01 08:30:00                588.00             42805.0   \n",
       "35 2018-10-01 08:45:00                759.50             42805.0   \n",
       "36 2018-10-01 09:00:00                958.50             42805.0   \n",
       "37 2018-10-01 09:15:00               1190.00             42805.0   \n",
       "38 2018-10-01 09:30:00               1388.75             42805.0   \n",
       "39 2018-10-01 09:45:00               1602.25             42805.0   \n",
       "40 2018-10-01 10:00:00               1795.00             42805.0   \n",
       "41 2018-10-01 10:15:00               2002.50             42805.0   \n",
       "42 2018-10-01 10:30:00               2214.75             42805.0   \n",
       "43 2018-10-01 10:45:00               2351.50             42805.0   \n",
       "44 2018-10-01 11:00:00               2419.75             42805.0   \n",
       "45 2018-10-01 11:15:00               2523.00             42805.0   \n",
       "46 2018-10-01 11:30:00               2622.75             42805.0   \n",
       "47 2018-10-01 11:45:00               2673.50             42805.0   \n",
       "48 2018-10-01 12:00:00               2751.75             42805.0   \n",
       "49 2018-10-01 12:15:00               2779.00             42805.0   \n",
       "\n",
       "    Total_consumption_MWh  DE_LU_price_per_MWh  normalisation_factor   \n",
       "0                10589.75                59.53              0.684015  \\\n",
       "1                10589.75                59.53              0.684015   \n",
       "2                10589.75                59.53              0.684015   \n",
       "3                10589.75                59.53              0.684015   \n",
       "4                10589.75                56.10              0.684015   \n",
       "5                10560.25                56.10              0.684015   \n",
       "6                10507.00                56.10              0.684015   \n",
       "7                10407.50                56.10              0.684015   \n",
       "8                10263.50                51.41              0.684015   \n",
       "9                10274.00                51.41              0.684015   \n",
       "10               10370.50                51.41              0.684015   \n",
       "11               10366.50                51.41              0.684015   \n",
       "12               10456.25                47.38              0.684015   \n",
       "13               10521.75                47.38              0.684015   \n",
       "14               10548.50                47.38              0.684015   \n",
       "15               10569.00                47.38              0.684015   \n",
       "16               10706.00                47.59              0.684015   \n",
       "17               10835.25                47.59              0.684015   \n",
       "18               10998.25                47.59              0.684015   \n",
       "19               11119.50                47.59              0.684015   \n",
       "20               11503.50                51.61              0.684015   \n",
       "21               11767.75                51.61              0.684015   \n",
       "22               12134.25                51.61              0.684015   \n",
       "23               12623.25                51.61              0.684015   \n",
       "24               13506.25                69.13              0.684015   \n",
       "25               14138.00                69.13              0.684015   \n",
       "26               14659.75                69.13              0.684015   \n",
       "27               15086.50                69.13              0.684015   \n",
       "28               15599.50                77.32              0.684015   \n",
       "29               15808.50                77.32              0.684015   \n",
       "30               15941.00                77.32              0.684015   \n",
       "31               16122.50                77.32              0.684015   \n",
       "32               16292.25                84.97              0.684015   \n",
       "33               16500.00                84.97              0.684015   \n",
       "34               16668.75                84.97              0.684015   \n",
       "35               16724.75                84.97              0.684015   \n",
       "36               16596.75                79.56              0.684015   \n",
       "37               16705.25                79.56              0.684015   \n",
       "38               16819.25                79.56              0.684015   \n",
       "39               16883.00                79.56              0.684015   \n",
       "40               16901.25                73.70              0.684015   \n",
       "41               16997.00                73.70              0.684015   \n",
       "42               17077.50                73.70              0.684015   \n",
       "43               17206.25                73.70              0.684015   \n",
       "44               17295.75                71.63              0.684015   \n",
       "45               17457.50                71.63              0.684015   \n",
       "46               17472.00                71.63              0.684015   \n",
       "47               17440.00                71.63              0.684015   \n",
       "48               17409.50                63.15              0.684015   \n",
       "49               17342.75                63.15              0.684015   \n",
       "\n",
       "    Solar_generation_MWh_normalized  \n",
       "0                          0.000000  \n",
       "1                          0.000000  \n",
       "2                          0.000000  \n",
       "3                          0.000000  \n",
       "4                          0.000000  \n",
       "5                          0.000000  \n",
       "6                          0.000000  \n",
       "7                          0.000000  \n",
       "8                          0.000000  \n",
       "9                          0.000000  \n",
       "10                         0.000000  \n",
       "11                         0.000000  \n",
       "12                         0.000000  \n",
       "13                         0.000000  \n",
       "14                         0.000000  \n",
       "15                         0.000000  \n",
       "16                         0.000000  \n",
       "17                         0.000000  \n",
       "18                         0.000000  \n",
       "19                         0.000000  \n",
       "20                         0.000000  \n",
       "21                         0.000000  \n",
       "22                         0.000000  \n",
       "23                         0.000000  \n",
       "24                         0.000000  \n",
       "25                         0.000000  \n",
       "26                         0.000000  \n",
       "27                         0.000000  \n",
       "28                         1.197027  \n",
       "29                        10.089227  \n",
       "30                        38.817874  \n",
       "31                        92.342080  \n",
       "32                       174.423928  \n",
       "33                       278.394270  \n",
       "34                       402.201058  \n",
       "35                       519.509700  \n",
       "36                       655.628765  \n",
       "37                       813.978331  \n",
       "38                       949.926393  \n",
       "39                      1095.963682  \n",
       "40                      1227.807651  \n",
       "41                      1369.740848  \n",
       "42                      1514.923117  \n",
       "43                      1608.462224  \n",
       "44                      1655.146275  \n",
       "45                      1725.770866  \n",
       "46                      1794.001402  \n",
       "47                      1828.715184  \n",
       "48                      1882.239389  \n",
       "49                      1900.878809  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163680 entries, 0 to 163679\n",
      "Data columns (total 7 columns):\n",
      " #   Column                           Non-Null Count   Dtype         \n",
      "---  ------                           --------------   -----         \n",
      " 0   Date                             163680 non-null  datetime64[ns]\n",
      " 1   Solar_generation_MWh             163680 non-null  float64       \n",
      " 2   Solar_installed_MW               163680 non-null  float64       \n",
      " 3   Total_consumption_MWh            163680 non-null  float64       \n",
      " 4   DE_LU_price_per_MWh              163680 non-null  float64       \n",
      " 5   normalisation_factor             163680 non-null  float64       \n",
      " 6   Solar_generation_MWh_normalized  163680 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6)\n",
      "memory usage: 8.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a lot of samples therefore I will limit the sample size for training a bit more\n",
    "#df = df.iloc[60000: , :]\n",
    "#len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df[['Total_consumption_MWh']], test_size=.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's scale the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Work in progress ...\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(input, n_steps, pred_size):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(input)):\n",
    "        end_ix = i + n_steps # find the end of this pattern\n",
    "        if end_ix+pred_size > len(input)-1: # check if we are beyond the sequence\n",
    "            break\n",
    "        seq_x, seq_y = input[i:end_ix], input[end_ix: end_ix+pred_size]# gather input and output parts of the pattern\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121992 121992\n"
     ]
    }
   ],
   "source": [
    "# define input sequence\n",
    "input = train\n",
    "# choose a number of time steps\n",
    "n_steps = 672\n",
    "\n",
    "# prediction size \n",
    "pred_size= 96\n",
    "# split into samples\n",
    "X, y = split_sequence(input, n_steps, pred_size)\n",
    "# summarize the data\n",
    "print(len(X), len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121992, 96, 1) (121992, 672, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have to define the validation set for our model #! I see this approach is not so useful, therfore I will use the train test split with shuffling to obtain the validation data. Here i am not loosing the lateest data for training my model \n",
    "def val_set(X,y):\n",
    "    X, X_val, y, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "    return X, X_val, y, y_val\n",
    "    #! old approach\n",
    "    #train_size = round(len(X) * 0.8)\n",
    "    #X = X[:train_size, :]\n",
    "    #X_val = X[train_size:, :]\n",
    "    #y = y[:train_size, :]\n",
    "    #y_val = y[train_size:, :]\n",
    "    \n",
    "X, X_val, y, y_val = val_set(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97593, 672, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "\n",
    "def reshape_for_LSTM(X, y, features):\n",
    "    features\n",
    "    X = X.reshape((X.shape[0], X.shape[1], features))\n",
    "    y = y.reshape((y.shape[0], y.shape[1]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = reshape_for_LSTM(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = reshape_for_LSTM(X_val, y_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24399, 672, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97593, 672, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets start the modeling approach using the Long short term memory model ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary to store results\n",
    "history = {}\n",
    "\n",
    "# Define number of epochs and learning rate decay\n",
    "N_TRAIN = len(X)\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 2371 # total sample size = 113808 each batch 2371 samples (48 batches ) #! has to be adjusted further to improve\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(  #! check the source \n",
    "    0.01,  #! please adjust and finetune\n",
    "    decay_steps=STEPS_PER_EPOCH*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "\n",
    "\n",
    "# Define optimizer used for modelling\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule, name='Adam')  # due to a warning message I used the legacy.Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path where checkpoints should be stored\n",
    "checkpoint_path = \"modeling/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0) # Set verbose != 0 if you want output during training \n",
    "\n",
    "#create a callback to stop early once there is no improvement in the loss\n",
    "cp_early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True,\n",
    "                                verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how many output layer are needed for predicting several timestamps? Please check one output layer is enough but some of the parameters have to be adjusted,\n",
    "\n",
    "n_steps, n_features\n",
    "X.shape[1], X.shape[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reason for not having activation functions https://datascience.stackexchange.com/questions/66594/activation-function-between-lstm-layers\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell\n",
    "\n",
    "output layer structure : https://stackoverflow.com/questions/46797891/output-shape-of-lstm-model#46799544\n",
    "\n",
    "https://shiva-verma.medium.com/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_LSTM_model():\n",
    "    simple_LSTM = tf.keras.Sequential([\n",
    "      tf.keras.layers.LSTM(45, kernel_initializer = 'uniform', input_shape = (X.shape[1], X.shape[2]), return_sequences=True), # ! units are not set in stone yet \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(y.shape[1] ,kernel_initializer = 'uniform', activation='linear') #96 to predict a day \n",
    "    ])\n",
    "\n",
    "    simple_LSTM.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.MeanAbsolutePercentageError(), \n",
    "                  metrics=[tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "    return simple_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 672, 45)           8460      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 672, 45)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                9984      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 96)                3168      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21612 (84.42 KB)\n",
      "Trainable params: 21612 (84.42 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    simple_LSTM = get_simple_LSTM_model()\n",
    "    print(simple_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 2: early stopping\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    history = simple_LSTM.fit(X,\n",
    "                        y,\n",
    "                        batch_size= BATCH_SIZE,\n",
    "                        validation_data= (X_val, y_val),   ##### probably best to make validation data D #! TO DO \n",
    "                        verbose=100,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[cp_callback, cp_early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we split the test set too \n",
    "X_test, y_test = split_sequence(test, n_steps, pred_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [105.76807403564453, 109.21354675292969],\n",
       " 'mean_absolute_percentage_error': [105.76810455322266, 109.24549865722656],\n",
       " 'val_loss': [105.07128143310547, 106.9535140991211],\n",
       " 'val_mean_absolute_percentage_error': [104.9910888671875, 106.83114624023438]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.24523597],\n",
       "        [ 1.23129074],\n",
       "        [ 1.24082165],\n",
       "        ...,\n",
       "        [ 0.85296359],\n",
       "        [ 0.81483994],\n",
       "        [ 0.8068139 ]],\n",
       "\n",
       "       [[ 1.23129074],\n",
       "        [ 1.24082165],\n",
       "        [ 1.23690896],\n",
       "        ...,\n",
       "        [ 0.81483994],\n",
       "        [ 0.8068139 ],\n",
       "        [ 0.79056119]],\n",
       "\n",
       "       [[ 1.24082165],\n",
       "        [ 1.23690896],\n",
       "        [ 1.22105755],\n",
       "        ...,\n",
       "        [ 0.8068139 ],\n",
       "        [ 0.79056119],\n",
       "        [ 0.75564794]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.99472959],\n",
       "        [-1.04910597],\n",
       "        [-1.11020414],\n",
       "        ...,\n",
       "        [-0.53523919],\n",
       "        [-0.61519854],\n",
       "        [-0.72625878]],\n",
       "\n",
       "       [[-1.04910597],\n",
       "        [-1.11020414],\n",
       "        [-1.19859084],\n",
       "        ...,\n",
       "        [-0.61519854],\n",
       "        [-0.72625878],\n",
       "        [-0.79317583]],\n",
       "\n",
       "       [[-1.11020414],\n",
       "        [-1.19859084],\n",
       "        [-1.25818413],\n",
       "        ...,\n",
       "        [-0.72625878],\n",
       "        [-0.79317583],\n",
       "        [-0.87433909]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "than we take only the first element of the splited test set and let the model predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.79056119],\n",
       "        [ 0.75564794],\n",
       "        [ 0.77340554],\n",
       "        [ 0.79507583],\n",
       "        [ 0.78143157],\n",
       "        [ 0.76588113],\n",
       "        [ 0.74230466],\n",
       "        [ 0.72916203],\n",
       "        [ 0.71732363],\n",
       "        [ 0.63896948],\n",
       "        [ 0.54877693],\n",
       "        [ 0.44865217],\n",
       "        [ 0.38374162],\n",
       "        [ 0.32735874],\n",
       "        [ 0.21088093],\n",
       "        [ 0.12399912],\n",
       "        [ 0.1251027 ],\n",
       "        [ 0.0761439 ],\n",
       "        [-0.04665441],\n",
       "        [-0.15721302],\n",
       "        [-0.23777433],\n",
       "        [-0.31051025],\n",
       "        [-0.43651898],\n",
       "        [-0.48838722],\n",
       "        [-0.57235959],\n",
       "        [-0.62001416],\n",
       "        [-0.69134553],\n",
       "        [-0.74100661],\n",
       "        [-0.75505217],\n",
       "        [-0.80130219],\n",
       "        [-0.84363951],\n",
       "        [-0.85878865],\n",
       "        [-0.89921979],\n",
       "        [-0.90744648],\n",
       "        [-0.89109344],\n",
       "        [-0.88988953],\n",
       "        [-0.85908963],\n",
       "        [-0.88958855],\n",
       "        [-0.87524202],\n",
       "        [-0.87203161],\n",
       "        [-0.8396265 ],\n",
       "        [-0.79136997],\n",
       "        [-0.77311075],\n",
       "        [-0.7496346 ],\n",
       "        [-0.71993828],\n",
       "        [-0.72956952],\n",
       "        [-0.73649197],\n",
       "        [-0.71873437],\n",
       "        [-0.66606353],\n",
       "        [-0.6718824 ],\n",
       "        [-0.68271755],\n",
       "        [-0.6461991 ],\n",
       "        [-0.55731078],\n",
       "        [-0.48337095],\n",
       "        [-0.36940127],\n",
       "        [-0.29315396],\n",
       "        [-0.18961813],\n",
       "        [-0.12641312],\n",
       "        [-0.01023628],\n",
       "        [ 0.08567481],\n",
       "        [ 0.17827517],\n",
       "        [ 0.22492649],\n",
       "        [ 0.31200895],\n",
       "        [ 0.37401006],\n",
       "        [ 0.42497537],\n",
       "        [ 0.43169717],\n",
       "        [ 0.4807563 ],\n",
       "        [ 0.51506759],\n",
       "        [ 0.47052311],\n",
       "        [ 0.50272757],\n",
       "        [ 0.55158605],\n",
       "        [ 0.5297151 ],\n",
       "        [ 0.48396671],\n",
       "        [ 0.41173242],\n",
       "        [ 0.42286854],\n",
       "        [ 0.37992926],\n",
       "        [ 0.34511634],\n",
       "        [ 0.2742866 ],\n",
       "        [ 0.23385545],\n",
       "        [ 0.14426486],\n",
       "        [ 0.0757426 ],\n",
       "        [ 0.00230439],\n",
       "        [-0.02990007],\n",
       "        [-0.08337351],\n",
       "        [-0.08267123],\n",
       "        [-0.0979207 ],\n",
       "        [-0.10664901],\n",
       "        [-0.1358437 ],\n",
       "        [-0.1401577 ],\n",
       "        [-0.08808881],\n",
       "        [-0.05889411],\n",
       "        [-0.10644836],\n",
       "        [-0.07053186],\n",
       "        [-0.04665441],\n",
       "        [-0.0075275 ],\n",
       "        [-0.00863108]],\n",
       "\n",
       "       [[ 0.75564794],\n",
       "        [ 0.77340554],\n",
       "        [ 0.79507583],\n",
       "        [ 0.78143157],\n",
       "        [ 0.76588113],\n",
       "        [ 0.74230466],\n",
       "        [ 0.72916203],\n",
       "        [ 0.71732363],\n",
       "        [ 0.63896948],\n",
       "        [ 0.54877693],\n",
       "        [ 0.44865217],\n",
       "        [ 0.38374162],\n",
       "        [ 0.32735874],\n",
       "        [ 0.21088093],\n",
       "        [ 0.12399912],\n",
       "        [ 0.1251027 ],\n",
       "        [ 0.0761439 ],\n",
       "        [-0.04665441],\n",
       "        [-0.15721302],\n",
       "        [-0.23777433],\n",
       "        [-0.31051025],\n",
       "        [-0.43651898],\n",
       "        [-0.48838722],\n",
       "        [-0.57235959],\n",
       "        [-0.62001416],\n",
       "        [-0.69134553],\n",
       "        [-0.74100661],\n",
       "        [-0.75505217],\n",
       "        [-0.80130219],\n",
       "        [-0.84363951],\n",
       "        [-0.85878865],\n",
       "        [-0.89921979],\n",
       "        [-0.90744648],\n",
       "        [-0.89109344],\n",
       "        [-0.88988953],\n",
       "        [-0.85908963],\n",
       "        [-0.88958855],\n",
       "        [-0.87524202],\n",
       "        [-0.87203161],\n",
       "        [-0.8396265 ],\n",
       "        [-0.79136997],\n",
       "        [-0.77311075],\n",
       "        [-0.7496346 ],\n",
       "        [-0.71993828],\n",
       "        [-0.72956952],\n",
       "        [-0.73649197],\n",
       "        [-0.71873437],\n",
       "        [-0.66606353],\n",
       "        [-0.6718824 ],\n",
       "        [-0.68271755],\n",
       "        [-0.6461991 ],\n",
       "        [-0.55731078],\n",
       "        [-0.48337095],\n",
       "        [-0.36940127],\n",
       "        [-0.29315396],\n",
       "        [-0.18961813],\n",
       "        [-0.12641312],\n",
       "        [-0.01023628],\n",
       "        [ 0.08567481],\n",
       "        [ 0.17827517],\n",
       "        [ 0.22492649],\n",
       "        [ 0.31200895],\n",
       "        [ 0.37401006],\n",
       "        [ 0.42497537],\n",
       "        [ 0.43169717],\n",
       "        [ 0.4807563 ],\n",
       "        [ 0.51506759],\n",
       "        [ 0.47052311],\n",
       "        [ 0.50272757],\n",
       "        [ 0.55158605],\n",
       "        [ 0.5297151 ],\n",
       "        [ 0.48396671],\n",
       "        [ 0.41173242],\n",
       "        [ 0.42286854],\n",
       "        [ 0.37992926],\n",
       "        [ 0.34511634],\n",
       "        [ 0.2742866 ],\n",
       "        [ 0.23385545],\n",
       "        [ 0.14426486],\n",
       "        [ 0.0757426 ],\n",
       "        [ 0.00230439],\n",
       "        [-0.02990007],\n",
       "        [-0.08337351],\n",
       "        [-0.08267123],\n",
       "        [-0.0979207 ],\n",
       "        [-0.10664901],\n",
       "        [-0.1358437 ],\n",
       "        [-0.1401577 ],\n",
       "        [-0.08808881],\n",
       "        [-0.05889411],\n",
       "        [-0.10644836],\n",
       "        [-0.07053186],\n",
       "        [-0.04665441],\n",
       "        [-0.0075275 ],\n",
       "        [-0.00863108],\n",
       "        [-0.01796134]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.08063494e-02 -5.13654836e-02  1.05123222e-02 -1.15628541e-03\n",
      "  -4.27013561e-02 -3.32875028e-02 -4.20079418e-02  1.21642891e-02\n",
      "   3.99669744e-02  1.30486460e-02 -8.74670129e-03  4.71927784e-03\n",
      "   5.42570837e-03 -4.32776287e-04 -4.69744205e-02 -2.13410240e-05\n",
      "   9.16296989e-03 -9.60112363e-03  6.62624184e-03 -5.12521565e-02\n",
      "   1.37399882e-04 -5.84100373e-02  5.19590043e-02 -4.48680855e-03\n",
      "  -2.68295594e-02 -3.92691046e-03  1.64784351e-03  4.27949056e-03\n",
      "   6.48635402e-02  4.90011647e-02 -5.30908350e-04 -3.27749923e-02\n",
      "   2.52108835e-03 -1.63261220e-03 -1.13625228e-02  1.41014028e-02\n",
      "   1.12632578e-02  3.91016155e-03  1.07045695e-02  4.40444238e-03\n",
      "  -3.26629449e-03 -1.17291929e-02  2.23706756e-03 -6.89699221e-03\n",
      "  -4.84339185e-02 -9.78859141e-03  2.21516546e-02  3.71545777e-02\n",
      "   2.70959875e-03  1.08263567e-02 -2.15003453e-03 -3.01081687e-04\n",
      "   1.56172924e-03  2.98223831e-03  2.19835415e-02  1.93621796e-02\n",
      "   4.59717074e-03 -1.05841160e-02  9.09958221e-03  3.65388580e-04\n",
      "   1.60910562e-02 -5.76802250e-03  3.49137496e-04 -1.46198133e-02\n",
      "  -1.00404788e-02 -3.49161681e-03  4.04362381e-03  6.40263259e-02\n",
      "  -3.25071439e-03 -6.11286610e-03  3.34486440e-02 -1.86136831e-03\n",
      "   3.87565568e-02  3.68117020e-02  1.18232789e-02  3.76914330e-02\n",
      "   2.15147901e-03 -2.29588449e-02  2.56815981e-02 -3.14660296e-02\n",
      "   1.44455675e-02  1.23096630e-02  8.34494084e-03 -1.94904376e-02\n",
      "  -3.99951451e-03  1.19519345e-02  1.54702924e-03  1.43516436e-03\n",
      "  -7.84140080e-04 -6.01694174e-03  2.89646499e-02  3.08498181e-02\n",
      "   2.61575505e-02  1.01465471e-02  2.66887229e-02  1.50522590e-03]\n",
      " [ 4.09868062e-02 -5.12600318e-02  1.05916774e-02 -1.06649846e-03\n",
      "  -4.27951813e-02 -3.32266800e-02 -4.21109386e-02  1.22884940e-02\n",
      "   3.99608910e-02  1.31781632e-02 -8.89039505e-03  4.65276279e-03\n",
      "   5.45821805e-03 -3.78619879e-04 -4.70514968e-02 -4.87314537e-06\n",
      "   9.29610059e-03 -9.71300527e-03  6.68282714e-03 -5.13206385e-02\n",
      "   5.42178750e-05 -5.84961474e-02  5.20213991e-02 -4.43540048e-03\n",
      "  -2.68367901e-02 -3.99981998e-03  1.64591195e-03  4.21565399e-03\n",
      "   6.50385916e-02  4.91580702e-02 -6.57766708e-04 -3.28738727e-02\n",
      "   2.47887755e-03 -1.69262104e-03 -1.12833269e-02  1.41685028e-02\n",
      "   1.13023110e-02  3.87273356e-03  1.06104668e-02  4.48836572e-03\n",
      "  -3.19719873e-03 -1.17681222e-02  2.17963965e-03 -6.89313468e-03\n",
      "  -4.86047231e-02 -9.73086618e-03  2.23664790e-02  3.71706113e-02\n",
      "   2.76459847e-03  1.08535476e-02 -2.24969466e-03 -3.68300825e-04\n",
      "   1.52286887e-03  2.97715422e-03  2.21122131e-02  1.90949105e-02\n",
      "   4.56666760e-03 -1.07917711e-02  9.12618451e-03  2.73788348e-04\n",
      "   1.59317926e-02 -5.71848266e-03  3.72377399e-04 -1.47196017e-02\n",
      "  -9.92348976e-03 -3.26523930e-03  3.83069366e-03  6.40480965e-02\n",
      "  -3.48139182e-03 -6.18419051e-03  3.33502144e-02 -1.75685296e-03\n",
      "   3.91369462e-02  3.73662859e-02  1.19654303e-02  3.77691463e-02\n",
      "   2.12355610e-03 -2.28424296e-02  2.58310139e-02 -3.14028189e-02\n",
      "   1.47039406e-02  1.25045925e-02  8.41007475e-03 -1.95910707e-02\n",
      "  -4.31992859e-03  1.21000046e-02  1.35964900e-03  1.53766014e-03\n",
      "  -7.15695322e-04 -6.06854074e-03  2.91194469e-02  3.11116017e-02\n",
      "   2.60610692e-02  1.01971235e-02  2.67769080e-02  1.72086246e-03]]\n"
     ]
    }
   ],
   "source": [
    "x_input = X_test[:2, :]\n",
    "x_input = x_input.reshape((x_input.shape[0], x_input.shape[1], 1))\n",
    "y_pred = simple_LSTM.predict(x_input, verbose=0)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_mse(y_test, y_pred):\n",
    "    return keras.metrics.mean_absolute_percentage_error(y_test.reshape(y_test.shape[0], y_test.shape[1]), y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([109.543564, 109.84453 ], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_mse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/saved_model/simple_LSTM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/saved_model/simple_LSTM/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire small model as a SavedModel.\n",
    "!mkdir -p ../models/saved_model\n",
    "simple_LSTM.save('../models/saved_model/simple_LSTM')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short summary ###\n",
    "model is performing shitty it looks like it cannot handle the seasonality at all "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach get rif of the seasonality beforehand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Consumption_detrend'] = (df['Total_consumption_MWh'] - df['Total_consumption_MWh'].shift(96))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163680 entries, 0 to 163679\n",
      "Data columns (total 8 columns):\n",
      " #   Column                           Non-Null Count   Dtype         \n",
      "---  ------                           --------------   -----         \n",
      " 0   Date                             163680 non-null  datetime64[ns]\n",
      " 1   Solar_generation_MWh             163680 non-null  float64       \n",
      " 2   Solar_installed_MW               163680 non-null  float64       \n",
      " 3   Total_consumption_MWh            163680 non-null  float64       \n",
      " 4   DE_LU_price_per_MWh              163680 non-null  float64       \n",
      " 5   normalisation_factor             163680 non-null  float64       \n",
      " 6   Solar_generation_MWh_normalized  163680 non-null  float64       \n",
      " 7   Consumption_detrend              163584 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(7)\n",
      "memory usage: 10.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113808 113808\n"
     ]
    }
   ],
   "source": [
    "#Split into test and train set\n",
    "train, test = train_test_split(df['Consumption_detrend'], test_size=.3, shuffle=False)\n",
    "\n",
    "# define input sequence\n",
    "input = train\n",
    "# choose a number of time steps (a week)\n",
    "n_steps = 672\n",
    "\n",
    "# prediction size (we want to predict a day)\n",
    "pred_size= 96\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(input, n_steps, pred_size)\n",
    "# summarize the data\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train set into train and validation set\n",
    "X, X_val, y, y_val = val_set(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = reshape_for_LSTM(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_LSTM_model():\n",
    "    wo_season_LSTM = tf.keras.Sequential([\n",
    "      tf.keras.layers.LSTM(45 ,kernel_initializer = 'uniform', input_shape = (X.shape[1], X.shape[2]), return_sequences=True), # ! units are not set in stone yet \n",
    "      tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(y.shape[1] ,kernel_initializer = 'uniform', activation='relu') #96 to predict a day \n",
    "    ])\n",
    "\n",
    "    wo_season_LSTM.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error')\n",
    "    return wo_season_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 672, 45)           8460      \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                9984      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 96)                3168      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21612 (84.42 KB)\n",
      "Trainable params: 21612 (84.42 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    wo_season_LSTM = get_season_LSTM_model()\n",
    "    print(wo_season_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/cpu:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     training_history[\u001b[39m'\u001b[39m\u001b[39mseason_free\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m wo_season_LSTM\u001b[39m.\u001b[39;49mfit(X,\n\u001b[1;32m      3\u001b[0m                         y,\n\u001b[1;32m      4\u001b[0m                         batch_size\u001b[39m=\u001b[39;49m BATCH_SIZE,\n\u001b[1;32m      5\u001b[0m                         validation_data\u001b[39m=\u001b[39;49m (X_val, y_val),   \u001b[39m##### probably best to make validation data D #! TO DO \u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m                         verbose\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m                         steps_per_epoch\u001b[39m=\u001b[39;49mSTEPS_PER_EPOCH,\n\u001b[1;32m      8\u001b[0m                         epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[1;32m      9\u001b[0m                         callbacks\u001b[39m=\u001b[39;49m[cp_callback, cp_early_stop])\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    training_history['season_free'] = wo_season_LSTM.fit(X,\n",
    "                        y,\n",
    "                        batch_size= BATCH_SIZE,\n",
    "                        validation_data= (X_val, y_val),   ##### probably best to make validation data D #! TO DO \n",
    "                        verbose=100,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[cp_callback, cp_early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history['season_free'].history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# short Summary \n",
    "sadly I still only get nan for loss and mse. check with this in detail \n",
    "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 40152\n  y sizes: 2\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEvaluate on test data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m results \u001b[39m=\u001b[39m wo_season_LSTM\u001b[39m.\u001b[39;49mevaluate(X_test, y_test, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2371\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest loss, test acc:\u001b[39m\u001b[39m\"\u001b[39m, results)\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/EES_Optimisation/.venv/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1950\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1943\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1944\u001b[0m         label,\n\u001b[1;32m   1945\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m   1946\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1947\u001b[0m         ),\n\u001b[1;32m   1948\u001b[0m     )\n\u001b[1;32m   1949\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1950\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 40152\n  y sizes: 2\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = wo_season_LSTM.evaluate(X_test, y_test, batch_size=2371)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
